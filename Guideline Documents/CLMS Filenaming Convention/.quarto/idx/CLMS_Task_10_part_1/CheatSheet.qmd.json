{"title":"A Cheatsheet for Developing Standards for Generative AI Training and Web Crawlers","markdown":{"yaml":{"title":"A Cheatsheet for Developing Standards for Generative AI Training and Web Crawlers","subtitle":"Information Provisioning for Generative Chatbots","author":"Ayan Chatterjee, Department of DIGITAL, NILU","date":"2024-10-29","sitemap":true,"toc":true,"toc-title":"Contents","toc-depth":3,"keywords":["AI standards","web crawlers","AI training","content formatting"],"format":{"html":"default"}},"headingText":"1. Introduction","containsRefs":false,"markdown":"\n\nThis document serve as a quick reference guide to ensure content follows structured formats essential for web crawlers and AI systems. Utilizing Quarto Markdown in HTMLs and generating sitemaps are critical for efficient crawling, helping search engines and AI models quickly index and retrieve well-structured content.\n\n\n## 1.1. Importance of Structured Data for AI and Web Crawlers\n\nGenerative AI and chatbots rely heavily on structured data to provide meaningful and accurate responses. For these systems to operate efficiently, they need access to data that is easy to index, retrieve, and process. Properly formatted content enables web crawlers and AI models to efficiently access and retrieve data, improving the accuracy of results provided to users.\n\nWeb crawlers, also known as bots or spiders, index web content by following hyperlinks. They require well-structured content, often formatted in HTML, with clear metadata to ensure content is discoverable and up-to-date for search engines and AI systems.\n\n------------------------------------------------------------------------\n\n## 1.2. Goals of Content Standardization\n\n-   **Improved Data Access**: Ensuring web crawlers and AI models can easily access structured data.\n-   **Enhanced Search Engine Optimization (SEO)**: Well-formatted content improves visibility and accessibility across search engines.\n-   **Better AI Model Training**: Consistent data structure helps in training models more effectively.\n-   **Faster Retrieval**: Structured content enables quicker retrieval of relevant information, especially in time-sensitive applications.\n\n------------------------------------------------------------------------\n\n## 1.3. Benefits of Sitemaps and Metadata\n\n-   **Sitemaps**: Provide a roadmap for web crawlers to discover all content. A well-structured sitemap enhances a crawler’s efficiency, ensuring that content is indexed properly.\n-   **Metadata**: Metadata improves the discoverability and accuracy of content retrieval. Metadata tags such as title, author, date, and description help crawlers and AI models understand the content’s structure and relevance.\n\n------------------------------------------------------------------------\n\n# 2. Content Standards for AI and Web Crawlers\n\n## 2.1. Content Structuring in Quarto Markdown\n\nQuarto Markdown provides an efficient way to structure content for generative AI and web crawlers. Use clear headings, subheadings, and metadata to help web crawlers navigate the content.\n\n### YAML Example for Metadata\n\n``` yaml\n---\ntitle: \"AI and Web Crawling Standards\"\nauthor: \"Your Name\"\ndate: \"2024-09-30\"\nkeywords: [\"AI standards\", \"web crawlers\", \"metadata\"]\nsitemap: true\n---\n```\n\n------------------------------------------------------------------------\n\n## 2.2. HTML Structuring for Web Crawlers\n\nSemantic HTML5 elements, such as `<article>`, `<section>`, and `<header>`, help web crawlers index and understand the content more efficiently.\n\n``` yaml\n---\n<article>\n  <header>\n    <h1>Understanding Web Crawlers</h1>\n    <meta name=\"description\" content=\"Overview of web crawlers and their role in AI training.\" />\n  </header>\n  <section>\n    <h2>How Web Crawlers Index Content</h2>\n    <p>Web crawlers use links and metadata to index the web.</p>\n  </section>\n</article>\n---\n```\n\n### 2.2.1. Microdata for Structured Content\n\n``` yaml\n---\n<article itemscope itemtype=\"https://schema.org/Article\">\n  <header>\n    <h1 itemprop=\"headline\">AI and Web Crawling</h1>\n    <meta itemprop=\"description\" content=\"Overview of AI training using web crawlers.\" />\n  </header>\n</article>\n---\n```\n\n------------------------------------------------------------------------\n\n## 2.3. PDF Structuring for AI Integration\n\nFor documents in PDF format, ensure proper tagging of sections and headings to improve readability and indexing by crawlers and AI models. Add relevant metadata to the document properties.\n\n``` yaml\n---\ntitle: \"Structured PDF for AI\"\nauthor: \"Your Name\"\nkeywords: [\"AI\", \"web crawlers\", \"PDF\"]\n---\n```\n\n------------------------------------------------------------------------\n\n## 2.4. HTML Structuring for AI Integration\n\nTo optimize content for AI integration, HTML documents should include semantic elements, structured data formats like JSON-LD, and relevant metadata. This helps AI systems process and train on the content efficiently.\n\n``` yaml\n---\n<article itemscope itemtype=\"https://schema.org/Article\">\n  <header>\n    <h1 itemprop=\"headline\">AI Training Data and Web Crawlers</h1>\n    <meta name=\"description\" content=\"How to structure content for AI training and web crawling.\" />\n  </header>\n  <section>\n    <h2>AI Model Training</h2>\n    <p>Semantic structure is essential for AI to understand content.</p>\n    <script type=\"application/ld+json\">\n    {\n      \"@context\": \"https://schema.org\",\n      \"@type\": \"Dataset\",\n      \"name\": \"AI Training Data\",\n      \"description\": \"Dataset structured for AI and web crawlers.\",\n      \"creator\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Your Organization\"\n      }\n    }\n    </script>\n  </section>\n</article>\n---\n```\n\n------------------------------------------------------------------------\n\n# 3. Importance of Sitemap Indexing in HTML Documents\n\nSitemaps are essential for enhancing the discoverability and accessibility of web content for both web crawlers and AI systems. As an XML file, a sitemap provides a structured roadmap of a website, listing URLs, metadata, and details like last modified dates and update frequency. This helps crawlers efficiently index content and enables generative AI models to train on well-structured data, improving processing and retrieval accuracy. Key Benefits of Sitemap Indexing for Web Crawling and AI Training are:\n\n-   **Improved Discoverability**: Sitemaps enable web crawlers to find all relevant resources on a site, especially for deep or hard-to-reach pages.\n\n-   **Efficient Crawling**: Crawlers can prioritize content based on metadata like the last updated date, making re-indexing more effective.\n\n-   **Structured Data for AI Training**: Well-indexed documents help generative AI models understand relationships between content, improving relevance and accuracy in AI-generated responses.\n\n-   **Faster Content Retrieval**: Sitemaps speed up indexing and ensure better search rankings, enabling faster content access for AI models.\n\n``` yaml\n---\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<url>\n    <loc>https://<your-username>.github.io/<your-repo-name>/index.html</loc>\n    <lastmod>2024-10-08T12:24:05Z</lastmod>\n    <changefreq>monthly</changefreq>\n    <priority>0.8</priority>\n</url>\n</urlset>\n---\n```\n\nSubmit your sitemap to search engines via tools like Google Search Console to ensure your content is indexed properly. This improves the discoverability of AI training datasets and documents by web crawlers and AI models.\n\n------------------------------------------------------------------------\n\n# 4. Best Practices for Information Formatting\n\n-   **Consistent Metadata:** Use uniform metadata (title, author, description, keywords) across all documents.\n\n-   **Structured Headings:** Organize content using headings and subheadings for easy navigation by both users and web crawlers.\n\n-   **Cross-references**: Link to related content to improve discoverability and create a cohesive data ecosystem.\n\n-   **Clear Language:** Use concise, non-technical language to ensure that both users and machines can understand the content.\n\n------------------------------------------------------------------------\n\n# 5. Quarto Markdown Editors\n\nTo work with Quarto Markdown (.qmd) files and have them generated automatically, we can use several editors that integrate well with Quarto. VS Code (Visual Studio Code), RStudio, JupyterLab with Quarto Integration, and Atom with Quarto Plugin  are some popular editors that support Quarto and can automatically generate .qmd files. \n\nR-Studio is lightweight, easy-to-use and integrates with Quarto and provides tools for rendering, previewing, and managing .qmd documents in an effective way.\n\n## Steps to Set It Up\n\n1. **Install RStudio**: Download from [RStudio](https://posit.co/download/rstudio-desktop/).\n2. **Install Quarto**: Follow [Quarto installation](https://quarto.org/docs/get-started/) instructions to install Quarto.\n3. **Create a New Quarto Document**:\n    - In RStudio, go to **File > New File > Quarto Document**.\n    - Choose the type of document you want (e.g., HTML, PDF, Word).\n    - A `.qmd` file will be created automatically.\n4. **Automatically Render `.qmd`**:\n    - After editing your document, you can preview it using **Render** or export it to various formats.\n\n## Benefits\n\n- Full support for Quarto with an integrated environment.\n- Provides tools for live preview and exporting.\n- Ideal for users familiar with R or data science workflows.\n\n\n------------------------------------------------------------------------\n\n# 6. Automation with GitHub Deployment\n\nAutomation is crucial for ensuring efficiency and consistency in the deployment of content structured for AI integration and web crawlers. By automating the rendering of Quarto Markdown, Markdown, and Jupyter Notebook files into HTML, generating a sitemap, and deploying the output to GitHub Pages, the process becomes seamless and repeatable with minimal human intervention. This ensures that any changes to content are instantly reflected on the website, keeping the content discoverable and up-to-date for web crawlers and AI systems. Steps in the Automation Pipeline are:\n\na.  **Trigger on Push or Pull Requests**:\n    -   The workflow is triggered whenever `.qmd` files are modified or included in a pull request, ensuring content is updated automatically. \nb.  **Checkout Repository**:\n    -   Retrieves the latest version of the repository where content resides.\nc.  **Install Quarto**:\n    -   Installs the necessary Quarto CLI to render files into HTML.\nd.  **Render Content**:\n    -   Converts Quarto Markdown, Markdown, and Jupyter Notebook files into HTML format for web deployment.\ne.  **Move Generated HTML to Deployment Folder**:\n    -   Organizes all generated HTML files into the designated folder (`docs`) for web deployment.\nf.  **Generate Sitemap**:\n    -   Automatically creates a `sitemap.xml` following the google structure and it helps search engines and web crawlers discover all available content on the website.\ng.  **Deploy to GitHub Pages**:\n    -   Deploys the `docs` folder, which contains the HTML and `sitemap.xml`, to GitHub Pages for public access.\n\n------------------------------------------------------------------------\n\n# 6. Conclusion\n\nStandardizing content formatting using Quarto Markdown, HTML5, and sitemaps is essential for enabling effective web crawling and AI training. Structured data ensures improved discoverability, faster indexing, and better accessibility, supporting the development of more accurate and responsive AI models.\n\n------------------------------------------------------------------------\n","srcMarkdownNoYaml":"\n\nThis document serve as a quick reference guide to ensure content follows structured formats essential for web crawlers and AI systems. Utilizing Quarto Markdown in HTMLs and generating sitemaps are critical for efficient crawling, helping search engines and AI models quickly index and retrieve well-structured content.\n\n# 1. Introduction\n\n## 1.1. Importance of Structured Data for AI and Web Crawlers\n\nGenerative AI and chatbots rely heavily on structured data to provide meaningful and accurate responses. For these systems to operate efficiently, they need access to data that is easy to index, retrieve, and process. Properly formatted content enables web crawlers and AI models to efficiently access and retrieve data, improving the accuracy of results provided to users.\n\nWeb crawlers, also known as bots or spiders, index web content by following hyperlinks. They require well-structured content, often formatted in HTML, with clear metadata to ensure content is discoverable and up-to-date for search engines and AI systems.\n\n------------------------------------------------------------------------\n\n## 1.2. Goals of Content Standardization\n\n-   **Improved Data Access**: Ensuring web crawlers and AI models can easily access structured data.\n-   **Enhanced Search Engine Optimization (SEO)**: Well-formatted content improves visibility and accessibility across search engines.\n-   **Better AI Model Training**: Consistent data structure helps in training models more effectively.\n-   **Faster Retrieval**: Structured content enables quicker retrieval of relevant information, especially in time-sensitive applications.\n\n------------------------------------------------------------------------\n\n## 1.3. Benefits of Sitemaps and Metadata\n\n-   **Sitemaps**: Provide a roadmap for web crawlers to discover all content. A well-structured sitemap enhances a crawler’s efficiency, ensuring that content is indexed properly.\n-   **Metadata**: Metadata improves the discoverability and accuracy of content retrieval. Metadata tags such as title, author, date, and description help crawlers and AI models understand the content’s structure and relevance.\n\n------------------------------------------------------------------------\n\n# 2. Content Standards for AI and Web Crawlers\n\n## 2.1. Content Structuring in Quarto Markdown\n\nQuarto Markdown provides an efficient way to structure content for generative AI and web crawlers. Use clear headings, subheadings, and metadata to help web crawlers navigate the content.\n\n### YAML Example for Metadata\n\n``` yaml\n---\ntitle: \"AI and Web Crawling Standards\"\nauthor: \"Your Name\"\ndate: \"2024-09-30\"\nkeywords: [\"AI standards\", \"web crawlers\", \"metadata\"]\nsitemap: true\n---\n```\n\n------------------------------------------------------------------------\n\n## 2.2. HTML Structuring for Web Crawlers\n\nSemantic HTML5 elements, such as `<article>`, `<section>`, and `<header>`, help web crawlers index and understand the content more efficiently.\n\n``` yaml\n---\n<article>\n  <header>\n    <h1>Understanding Web Crawlers</h1>\n    <meta name=\"description\" content=\"Overview of web crawlers and their role in AI training.\" />\n  </header>\n  <section>\n    <h2>How Web Crawlers Index Content</h2>\n    <p>Web crawlers use links and metadata to index the web.</p>\n  </section>\n</article>\n---\n```\n\n### 2.2.1. Microdata for Structured Content\n\n``` yaml\n---\n<article itemscope itemtype=\"https://schema.org/Article\">\n  <header>\n    <h1 itemprop=\"headline\">AI and Web Crawling</h1>\n    <meta itemprop=\"description\" content=\"Overview of AI training using web crawlers.\" />\n  </header>\n</article>\n---\n```\n\n------------------------------------------------------------------------\n\n## 2.3. PDF Structuring for AI Integration\n\nFor documents in PDF format, ensure proper tagging of sections and headings to improve readability and indexing by crawlers and AI models. Add relevant metadata to the document properties.\n\n``` yaml\n---\ntitle: \"Structured PDF for AI\"\nauthor: \"Your Name\"\nkeywords: [\"AI\", \"web crawlers\", \"PDF\"]\n---\n```\n\n------------------------------------------------------------------------\n\n## 2.4. HTML Structuring for AI Integration\n\nTo optimize content for AI integration, HTML documents should include semantic elements, structured data formats like JSON-LD, and relevant metadata. This helps AI systems process and train on the content efficiently.\n\n``` yaml\n---\n<article itemscope itemtype=\"https://schema.org/Article\">\n  <header>\n    <h1 itemprop=\"headline\">AI Training Data and Web Crawlers</h1>\n    <meta name=\"description\" content=\"How to structure content for AI training and web crawling.\" />\n  </header>\n  <section>\n    <h2>AI Model Training</h2>\n    <p>Semantic structure is essential for AI to understand content.</p>\n    <script type=\"application/ld+json\">\n    {\n      \"@context\": \"https://schema.org\",\n      \"@type\": \"Dataset\",\n      \"name\": \"AI Training Data\",\n      \"description\": \"Dataset structured for AI and web crawlers.\",\n      \"creator\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Your Organization\"\n      }\n    }\n    </script>\n  </section>\n</article>\n---\n```\n\n------------------------------------------------------------------------\n\n# 3. Importance of Sitemap Indexing in HTML Documents\n\nSitemaps are essential for enhancing the discoverability and accessibility of web content for both web crawlers and AI systems. As an XML file, a sitemap provides a structured roadmap of a website, listing URLs, metadata, and details like last modified dates and update frequency. This helps crawlers efficiently index content and enables generative AI models to train on well-structured data, improving processing and retrieval accuracy. Key Benefits of Sitemap Indexing for Web Crawling and AI Training are:\n\n-   **Improved Discoverability**: Sitemaps enable web crawlers to find all relevant resources on a site, especially for deep or hard-to-reach pages.\n\n-   **Efficient Crawling**: Crawlers can prioritize content based on metadata like the last updated date, making re-indexing more effective.\n\n-   **Structured Data for AI Training**: Well-indexed documents help generative AI models understand relationships between content, improving relevance and accuracy in AI-generated responses.\n\n-   **Faster Content Retrieval**: Sitemaps speed up indexing and ensure better search rankings, enabling faster content access for AI models.\n\n``` yaml\n---\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<url>\n    <loc>https://<your-username>.github.io/<your-repo-name>/index.html</loc>\n    <lastmod>2024-10-08T12:24:05Z</lastmod>\n    <changefreq>monthly</changefreq>\n    <priority>0.8</priority>\n</url>\n</urlset>\n---\n```\n\nSubmit your sitemap to search engines via tools like Google Search Console to ensure your content is indexed properly. This improves the discoverability of AI training datasets and documents by web crawlers and AI models.\n\n------------------------------------------------------------------------\n\n# 4. Best Practices for Information Formatting\n\n-   **Consistent Metadata:** Use uniform metadata (title, author, description, keywords) across all documents.\n\n-   **Structured Headings:** Organize content using headings and subheadings for easy navigation by both users and web crawlers.\n\n-   **Cross-references**: Link to related content to improve discoverability and create a cohesive data ecosystem.\n\n-   **Clear Language:** Use concise, non-technical language to ensure that both users and machines can understand the content.\n\n------------------------------------------------------------------------\n\n# 5. Quarto Markdown Editors\n\nTo work with Quarto Markdown (.qmd) files and have them generated automatically, we can use several editors that integrate well with Quarto. VS Code (Visual Studio Code), RStudio, JupyterLab with Quarto Integration, and Atom with Quarto Plugin  are some popular editors that support Quarto and can automatically generate .qmd files. \n\nR-Studio is lightweight, easy-to-use and integrates with Quarto and provides tools for rendering, previewing, and managing .qmd documents in an effective way.\n\n## Steps to Set It Up\n\n1. **Install RStudio**: Download from [RStudio](https://posit.co/download/rstudio-desktop/).\n2. **Install Quarto**: Follow [Quarto installation](https://quarto.org/docs/get-started/) instructions to install Quarto.\n3. **Create a New Quarto Document**:\n    - In RStudio, go to **File > New File > Quarto Document**.\n    - Choose the type of document you want (e.g., HTML, PDF, Word).\n    - A `.qmd` file will be created automatically.\n4. **Automatically Render `.qmd`**:\n    - After editing your document, you can preview it using **Render** or export it to various formats.\n\n## Benefits\n\n- Full support for Quarto with an integrated environment.\n- Provides tools for live preview and exporting.\n- Ideal for users familiar with R or data science workflows.\n\n\n------------------------------------------------------------------------\n\n# 6. Automation with GitHub Deployment\n\nAutomation is crucial for ensuring efficiency and consistency in the deployment of content structured for AI integration and web crawlers. By automating the rendering of Quarto Markdown, Markdown, and Jupyter Notebook files into HTML, generating a sitemap, and deploying the output to GitHub Pages, the process becomes seamless and repeatable with minimal human intervention. This ensures that any changes to content are instantly reflected on the website, keeping the content discoverable and up-to-date for web crawlers and AI systems. Steps in the Automation Pipeline are:\n\na.  **Trigger on Push or Pull Requests**:\n    -   The workflow is triggered whenever `.qmd` files are modified or included in a pull request, ensuring content is updated automatically. \nb.  **Checkout Repository**:\n    -   Retrieves the latest version of the repository where content resides.\nc.  **Install Quarto**:\n    -   Installs the necessary Quarto CLI to render files into HTML.\nd.  **Render Content**:\n    -   Converts Quarto Markdown, Markdown, and Jupyter Notebook files into HTML format for web deployment.\ne.  **Move Generated HTML to Deployment Folder**:\n    -   Organizes all generated HTML files into the designated folder (`docs`) for web deployment.\nf.  **Generate Sitemap**:\n    -   Automatically creates a `sitemap.xml` following the google structure and it helps search engines and web crawlers discover all available content on the website.\ng.  **Deploy to GitHub Pages**:\n    -   Deploys the `docs` folder, which contains the HTML and `sitemap.xml`, to GitHub Pages for public access.\n\n------------------------------------------------------------------------\n\n# 6. Conclusion\n\nStandardizing content formatting using Quarto Markdown, HTML5, and sitemaps is essential for enabling effective web crawling and AI training. Structured data ensures improved discoverability, faster indexing, and better accessibility, supporting the development of more accurate and responsive AI models.\n\n------------------------------------------------------------------------\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"output-file":"CheatSheet.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","editor":"visual","preview":{"browser":false,"port":8888},"chrome":null,"theme":"flatly","title":"A Cheatsheet for Developing Standards for Generative AI Training and Web Crawlers","subtitle":"Information Provisioning for Generative Chatbots","author":"Ayan Chatterjee, Department of DIGITAL, NILU","date":"2024-10-29","sitemap":true,"toc-title":"Contents","keywords":["AI standards","web crawlers","AI training","content formatting"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","pdf","docx"]}